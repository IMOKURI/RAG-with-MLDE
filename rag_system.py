import logging
import math
import os
import time

import openai
import streamlit as st
from langchain.llms import OpenAI

from rag_utils import DocumentDB, EmbeddingModel, IndexDB


def as_minutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return "%dm %ds" % (m, s)


def time_since(since):
    now = time.time()
    s = now - since
    return f"Run time: {as_minutes(s)}"


def main():
    logging.basicConfig(level=logging.INFO)

    model_names = [
        # "cl-nagoya/sup-simcse-ja-large",
        # "intfloat/multilingual-e5-large",
        # "pkshatech/GLuCoSE-base-ja",
        "studio-ousia/luke-japanese-large",
    ]

    db_dir = "/app/rag-system/db"

    st.set_page_config(page_title="ğŸ˜ Quickstart App")

    st.title("ğŸ˜ Quickstart App")
    st.write("Hello world!")

    # openai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")
    openai_api_key = "dummy"
    openai_api_base = "http://fastchat-api-server:8000/v1"

    openai.api_key = openai_api_key
    openai.api_base = openai_api_base

    with st.form("rag_form"):
        text = st.text_area("Enter text:", "ãƒ„ã‚¤ãƒƒã‚¿ãƒ¼ãŒæœ€è¿‘è¡Œã£ãŸèª¿æŸ»ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚")
        submitted = st.form_submit_button("Submit")

        if submitted:
            start = time.time()

            models = []
            index_dbs = []
            document_dbs = []

            for model_name in model_names:
                models.append(EmbeddingModel(model_name))

                model_name = model_name.replace("/", "_")

                embedding_db_path = os.path.join(db_dir, f"{model_name}_embedding.index")
                index_dbs.append(IndexDB(embedding_db_path))

                document_db_path = os.path.join(db_dir, f"{model_name}_document.db")
                document_dbs.append(DocumentDB(document_db_path))
                logging.info(f"Loaded {model_name} ... {time_since(start)}")

            llm = OpenAI(openai_api_key=openai_api_key, openai_api_base=openai_api_base, batch_size=1)
            logging.info(f"Loaded LLM ... {time_since(start)}")

            documents = []
            for model, index_db, document_db in zip(models, index_dbs, document_dbs):
                embedded_text = model.embedding(text)

                indeices = index_db.search(embedded_text, 1)
                documents += document_db.search(indeices)

            prompt = (
                "ã‚ãªãŸã¯ä¸–ç•Œä¸­ã§ä¿¡é ¼ã•ã‚Œã¦ã„ã‚‹QAã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚\n"
                "äº‹å‰çŸ¥è­˜ã§ã¯ãªãã€å¸¸ã«æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
                "å¾“ã†ã¹ãã„ãã¤ã‹ã®ãƒ«ãƒ¼ãƒ«:\n"
                "1. å›ç­”å†…ã§æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥å‚ç…§ã—ãªã„ã§ãã ã•ã„ã€‚\n"
                "2. ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€...ã€ã‚„ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã¯...ã€ã€"
                "ã¾ãŸã¯ãã‚Œã«é¡ã™ã‚‹ã‚ˆã†ãªè¨˜è¿°ã¯é¿ã‘ã¦ãã ã•ã„ã€‚\n"
                "3. 200æ–‡å­—ç¨‹åº¦ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
                "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚\n"
                "---------------------\n" + "\n---------------------\n".join(documents) + "\n---------------------\n"
                "äº‹å‰çŸ¥è­˜ã§ã¯ãªãã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è€ƒæ…®ã—ã¦ã€ã‚¯ã‚¨ãƒªã«ç­”ãˆã¾ã™ã€‚\n"
                "ç–‘å•ãŒã‚ã‚‹å ´åˆã¯ã€ã€Œæƒ…å ±ç„¡ã—ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚\n"
                f"Query: {text}\n"
                "Answer: "
            )
            st.write("Reference document")
            st.info(documents[0])
            logging.info(f"Searched index ... {time_since(start)}")

            st.write("LLM Response")
            st.info(llm(prompt))

            st.write(time_since(start))
            logging.info(f"Returned answer ... {time_since(start)}")


if __name__ == "__main__":
    main()
