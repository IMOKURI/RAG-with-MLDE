name: batch-inference
workspace: batch-inference
project: embedding
environment:
  environment_variables:
    - http_proxy=http://hpeproxy.its.hpecorp.net:8080/
    - https_proxy=http://hpeproxy.its.hpecorp.net:8080/
    - no_proxy=localhost,127.0.0.1,ponkots01,16.171.32.68,10.0.0.0/8,192.168.0.0/16,172.16.0.0/16
  #   - NCCL_DEBUG=INFO
  image:
    gpu: localhost:32000/determined:latest
  force_pull_image: false
bind_mounts:
  - host_path: /data/home/sugiyama/rag-system
    container_path: rag-system
  - host_path: /home/sugi/.cache
    container_path: /root/.cache
resources:
  slots_per_trial: 1
  shm_size: 274877906944
max_restarts: 0
profiling:
  enabled: true
  begin_on_batch: 0
  end_after_batch: null
  sync_timings: false
searcher:
  name: single
  max_length: 100
  metric: x
entrypoint: >-
  python -m determined.launch.torch_distributed
  python embedding_generation.py
